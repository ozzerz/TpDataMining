{
  "comments": [
    "Improve file encoding support. Eclipse 2.1 uses a single global file encoding \nsetting for reading and writing files in the workspace. This is problematic; \nfor example, when Java source files in the workspace use OS default file \nencoding while XML files in the workspace use UTF-8 file encoding. The \nPlatform should support non-uniform file encodings. [Platform Core, Platform \nUI, Text, Search, Compare, JDT UI, JDT Core] [Theme: User experience]",
    "*** Bug 36950 has been marked as a duplicate of this bug. ***",
    "Original PR: bug 5399.",
    "Re: Non-uniform file encodings in the Eclipse Platform\n\nMany worthwhile ideas here. Other comments...\n\n1. I assume in the \"basic algorithm\" steps are performed in order listed. In\nthat case, steps 2 and 3 must be interchanged. The encoding interpreter must\nalways be consulted first. Multiple encodings are possible with the same BOM.\nThe result of (current) step 3 should be final. Otherwise, the BOM test should\nbe ignored unless it is inconsistent with the result of step 4 or 5.\n\n2. Encoding must be determined upon save as well as open. This determination may\nrequire calling an output encoding interpreter, which you do not have in your\nscheme. (Use case: User has an \u003c?xml encoding declaration in an XML file and\nchanges text of the encoding attribute.) The editor should not be required to\ntrack these changes character-by-character and blast off encoding change\nnotifications. In fact, the editor may not be aware of encoding at all. (Use\ncase: Rick Jellife has proposed an encoding declaration that would appear in\ncomments at the beginning of a file.) Instead, an output encoding interpreter\nshould be called at save time. IOW, the \"basic algorithm\" should be applied at\nsave time, too, using an encoding interpreter that operates on the Unicode text\ninstead of a byte stream.\n\n3. In light of the above, notifying of encoding changes seems of limited value,\nsince may be re-determined at open/save time. Encoding should be discovered when\nit is needed. Notification may be counter-productive, leading editors to take\nactions they should not be taking, like calling setCharset().\n\n4. setEncoding() should be removed and the basic algorithm should be the\ndescription of how getEncoding() works. setEncoding() is a potential source of\nproblems. For example, if setEncoding() is called on an open resource and the\nresource is then saved and closed, the resource cannot be re-opened successfully\nunless the encoding set is remembered. This makes it a resource property, but\nthere is already a resource property that may contain an encoding and the two\nmay be in conflict. What is a valid use of setEncoding()?\n\n5. It should be possible for an editor to have associated encoding\ninterpreter(s), so that the user is not forced to set the encoding interpreter\nand the editor separately. It is highly likely that the user will not be aware\nof the encoding interpreter feature and will not correctly set it in advance of\nhaving encoding problems. In fact, users seem to have problems learning how to\nset editors associated with extensions, and they already know what an editor is.\nLikewise, editors should not have to establish their own encoding interpreters\nprogrammatically.\n\n6. What is the use/purpose of isDefaultEncoding()? There may be several\n\"defaults\". If anyone cares that a resource is not using the workspace-level\nencoding, they should stop caring.\n\n7. Workspace-level, resource-level and interpreter-determined are requirements,\nbut I am not convinced there are use cases to support directory-level encoding,\nand they do add overhead. If the feature exists, someone may find it useful, if\nthat\u0027s the threshhold.",
    "I\u0027ll give a couple high level comments, though not an exhaustive list (just \nwanted to document some main things first). \n\n1. I agree with Bob\u0027s comment that the BOM step should be done first, but my \nmemory of the standards is that step should be final, if there is a BOM. That \nis, I thought the BOM was definitive. I\u0027m not aware of cases where \"Multiple \nencodings are possible with the same BOM\". Bob, perhaps you can explain?\n\n2. Encoding (interpreters) definitely needs to be associated with content-type, \nnot file extension. That seems to be assumed, understood by everyone, but just \nwanted to add my voice to the importance of that. \n\n3. While an \"output\" interpreter would also be necessary, I think some token \nremembering the encoding used during input (or the last output) is required \ntoo. For one thing, if a 3 byte BOM (for UTF-8) is detected during read, its \nonly polite to maintain that when written. For a second thing, which does \ndepend on the token being \"kept up to date\" with notifications, it is possible \nthat someone can paste text into a document that \"violates\" the encoding. Some \nwell behaved editors might want to give some warning about that. \n\n4. I might be explicit that the above comment assumes EncodingMemento (my name \nfor the token :) should be associated not only with the IFile/Resouce, but also \nthe IDocument. It is, after all, possible to save/copy a document independently \nof its original resource. In those cases the encoding token should ride along. \n\n5. Something that seems missing from the spec is the association of IANA \nencoding names and Java encoding names. I suggest this be provided at a \"base\" \nlevel since 1) there\u0027s some ambiguities, 2) its dependent on VM and platform, \nand 3) should be a \"base\" preference that allows users to control that \nassociation, when needed. (Most users don\u0027t need to do this, but some do, and \nfor those that do, there\u0027s no alternative workaround).\n\n6. I suspect there\u0027s a few well known interpreters that should be included as \npart of the base support: XML, at least. HTML, JSP, CSS also come to mind. \nOthers?\n\n7. The spec also doesn\u0027t mention how conflict resolution between interpreters \nis handled. I suspect that if the \"well known\" ones were included as base \nsupport, there\u0027d be little need beyond a warning message in the log file, but \nif, for example, everyone needs to re-invent an XML interpreter, there\u0027d be \nplenty of opportunity for conflict and users may desire a choice as to which \nwas used (which would be unfortunate). \n\n8. Also not well covered in the spec is exception handling. From experience, \nits easy for a file (e.g. an XML file) to specify an encoding but that some \ncharacter(s) in the file don\u0027t actual use that encoding, and a \"MalFormedInput\" \nexception will be raised. Some mechanism is then needed to allow users and/or \nclient code to \"override\" whatever the \"default\" behavior should be. For \nexample, an editor might want to give the user a choice to \"use default\" or \npick another encoding to try. \n\n9. The above point reminds me that in Java 1.4 there\u0027s an encoding setting that \nallows different behavior on encoding errors during input. One option throws an \nexception, the other substitutes \u0027?\u0027 for unreadable characters (well, actually, \nthey say they make an attempt to \"guess\" what the character is, but I\u0027m not \nsure that\u0027s very accurate). The point being that some parallel setting should \nexist with base Eclipse support, which would then \"pass through\" to the \nunderlying Java support. There\u0027s a similar situation with encoding errors on \noutput, but a little worse. Even with 1.3, invalid characters are written \nas \u0027?\u0027 instead of automatically throwing an exception, and some care is needed \nto handle invalid characters on output. The degree of care, I suggest, should \nalso be \"settable\" by client code. ",
    "David -\n\nThis is a longish response, though I agree with most of your points.\n\n1/3a. I have a general concern that Eclipse doesn\u0027t quite \"get it\" when it comes\nto plug-ins. In case after case, we find super-privileged plug-ins that are\nallowed to get in first and establish default behaviors by means that do not\nfollow extension point rules and are difficult or impossible for other plug-ins\nto override. Much of 3.0 is an effort to correct this problem for menus and\ntoolbars; the 2.1 cycle tried to do the same for keyboard shortcuts. Whenever I\nsee some \"default\" behavior making irrevocable decisions without consulting\nplug-ins, I get nervous.\n\nFor example, according to Unicode, the BOM is definitive and since the BOM was\nwritten by the last writer or agent, it probably reflects the current encoding.\nSo it would be useful to see the encoding interpreter plug-in called with an\nargument that indicates the BOM encoding (or none), as well as the length of the\nBOM.\n\nThere is no harm, either, in recording the BOM-determined encoding in a memento,\nprovided the mememto clearly indicates the origin of the information. To carry\nthis information around as some sort of vague \"preferred encoding\" would be\ncounter-productive.\n\nWhen a document is modified and saved, or created and saved for the first time,\nthe encoding indicated within the document, if any, must always be consulted.\nThe precedence for determining the write encoding should be:\n\n1. In-document encoding, if any.\n2. Resource property encoding, if any.\n3. Directory default encoding(s), if any.\n4. Platform encoding\n\nIf any of these is different than the input BOM encoding, the plug-in should ask\nthe user to confirm or switch to the input encoding. The input encoding really\nhas no privileged relationship to the output encoding, but there are two good\nreasons to ask:\n\n- One of the default encodings may lose information.\n- Users most often don\u0027t know what the platform default encoding is (at least,\nuntil they are screwed the first time).\n\n2. Having done it this way in another product because I thought it was the right\nanswer, I am keenly aware of two issues:\n\n- Users really don\u0027t understand content types and don\u0027t appreciate the added\nlevel of indirection (extension -\u003e content type -\u003e encoding\ninterpreter/editor/whatever). Especially note the editor/whatever part.\n\n- If by content type you mean MIME type, it doesn\u0027t mean and doesn\u0027t uniquely\nidentify \"document type\". E.g., text/xml and application/xml are the same\ndocument type with allegedly different encoding (but don\u0027t count on it). If you\ndon\u0027t mean MIME type, then I don\u0027t know what you mean, but inventing a new\ndocument type naming convention is, as they say, fraught.\n\n3a. Can\u0027t say that ignoring all of the user\u0027s encoding preferences and using\ninstead an encoding the user can\u0027t see is \"polite\". Seems downright rude.\n\n3b. The editor is going to dynamically track changes to the many encoding\npreferences, which requires resolving to the single relevant encoding preference\neach time, so that it can check the entire document and every document change\nbetween preference changes to make sure it is within that encoding? I don\u0027t\nthink so. Or at least, I hope not. It might be valuable to check the document on\nsave to ensure that information won\u0027t be lost - not needed very often, but\ncertainly handy when it is, but that check can be non-trivial (and unfortunately\nis most useful when it is non-trivial) and is quite beyond the resources of most\neditor-writers.\n\n4. I\u0027m not sure that copying the contents of a document, which are\nencoding-neutral, has anything to do with how the copy is treated afterward.\n\n5. I don\u0027t recall that Java is so perverse that it doesn\u0027t recognize any IANA\nnames, nor that it gives a different interpretation to any. I certainly could be\nwrong, but that seems more like material for a bug report to Sun. What I do is\npresent choices only in terms of IANA names but attempt to map any name provided\nfirst into a Java name otherwise to an IANA name, using maps that are not\ncase-sensitive and accomodate common variations in punctuation, otherwise in a\ndialog I will flag an error; if an possibly invalid name appears in a document,\nI try the name anyway. I don\u0027t consider it possible to predict what names Java\nwill accept in any given release.\n\nThen what to do if Java rejects a name? I could probably do this better, but\nhere\u0027s what I currently do: If Java rejects a user-specified encoding on save, I\nsave in UTF-8 and depend on the platform to write a BOM. If the platform\nco-operates, then no information is lost, the document is always readable and\nsave always succeeds. I felt that save succeeding was more important than user\nnotification (or trying to choose another preference, which might again result\nin user notification). To allow a user to do a Save All and walk away without\nseeing some dialog pop up, and later experience a power failure, seemed to me to\nbe a hanging offense. That said, some user notification after the fact, and some\nassurance that the BOM is actually written, would improve matters.\n\n6/7. All encoding interpreters should be treated equally; the\nplatform-contributed ones should be just like any other. There needs to be a\nresultion algorithm, which was the source of my previous comment that if a user\nselects an editor for a document and the editor contributes an encoding\ninterpreter, that interpreter ought to be used. It is really asking a bit much\nfor users to deal with this as a separate issue, and I don\u0027t care that files can\nbe opened by plug-ins that are not editors, this use case is important enough to\nget special treatment.\n\nAs always, the resolution algorithm should take into account the important use\ncases. Open in editor and Search come readily to mind. (Maybe Search is simple:\nthe contents should be obtained from already open documents, if possible,\notherwise apply the algorithm.)\n\n8. I agree for Open in editor the user should be given a choice on input if a\nspecified encoding throws; I\u0027m not so sure for Search; probably this should just\ngo into the status messages and let the search continue without the file. Use\ncases rule.\n\n9. For encoding, the default should be try/catch/recover for both input and\noutput. Font/code page selection is also involved. The most common report I see\nis that the file is read correctly but the user sees boxes or garbage, and\nthat\u0027s often a presentation problem. The current one-font-fits-all behavior\ndoesn\u0027t really cut it, any more than one-encoding-fits-all does. It seems\nobvious that the correct presentation can, and should, be selected based on the\nencoding if it is, say, ASCII or SHIFT-JIS, but not obvious if it is UTF-8. But\nall this is platform-dependent and not one of my areas of expertise.",
    "Actually, WRT 3b, the determination is trivial enough if you try the translation\nand it throws.",
    "Thought I should add some comments on Bob\u0027s comments. Hopefully, we\u0027ll approach \nsome clear statements of principles or uses cases, and then could decide if new \nencoding support can/will support those use cases. \n\nFirst, on 3b. ... I hate to be thought of as rude :) so I\u0027ll clarify I \noriginally meant ... given a resource was read in as UTF8 (and it had a 3 byte \nBOM) then if the resource is written as UTF8, its only polite to also include \nthe 3 byte BOM when written. But, if read in as UTF8 (and did not have 3 byte \nBOM) then if the resource is written as UTF8, it is only appropriate to not \ninclude the 3 byte BOM. And, my point was, the only way to know what to do about \nthe 3 byte BOM is to \"carry along\" that \"how read\" info, such as in an \nEncodingMemento. \n\nAt more of a \"principle\" level, I do think it important to carry along the whole \nencoding info that a resource was read with. And, I think there should be a rule \nthat says \"in the absence of over-riding information, a resource should be \nwritten as it was read. The \"over-riding\" part would be rules 1 and 2 in Bob\u0027s \nlist, so I guess this would be a \"rule 2.1\", coming before \"directory setting\". \nThe \"use cases\" in support of this principle are easy to find: a. If a resource \nis read in with unicode encoding due to some BOM (3 or 4 (or more?) bytes) AND \nit is not otherwise specified, then I think it should be written out the same \nway. I think that\u0027s the intent of the unicode standard BOM, though I don\u0027t know \nin practice how many people use this technique (since most \u0027modern files\u0027 would \ncontain the encoding in the file itself (e.g. for XML, etc.). [Guess it would \napply to Java files! :) b. Another use case, I\u0027ve personally seen, is that there \nare some cases for HTML files (which is not so standard on encoding spec\u0027s) \nthere are some Japanese systems which \"peek\" inside the file to determine \nencoding (peek in the sense of looking at byte patterns) and if one of those \nJapanese encodings is found, those users would expect (require!) it to be \nwritten the same way. \n\nAnd, this above use cases is the reason why its important to \"carry along\" the \nencoding info through to IDocument (or similar), and not just resources, so that \nif a resource from one of the above cases results in an IDocument, and that \nIDocument is cloned/copied/savedAs... some other resource, then it would have \nthe same characteristics as the original. \n\nJust a quick note on the importance of associating encoding rules with \"content \ntype\" ... the prima-dona use case for this is JSPs. Typically using a file \nextension of .jsp, there\u0027s no reason why any user can\u0027t change the settings on \ntheir web server, and want \u0027jst\u0027 to also be interpreted as a JSP file. They \nshould then have an easy way to let the tool/platform know that \u0027jst\u0027 should be \nin the JSP \"family\" (is that a better term?) and then have everything in the \nplatform that in some way handles .jsp files handle .jst files in the same way \n(not just editor association). \n\nLastly, and what may be a different view between Bob and I, is that I do see \nencoding/decoding only working correctly, from a platform point of view, if the \nresolution algorithm always results in the same interpreter/ rule being used to \ndo encoding/decoding. The reason for this is that there are so many functions \n(compilers, builders, validaters, databases, search indices, fixups, code \ngenerators, etc.) that all depend on the resource being \"interpreted\" the same \nway. Of course, this should not preclude some functions (e.g. editors) from \nchanging the encoding/decoding, but I think this should be a separate \"concept\" \nfrom the \"interpreter resolution\". In fact, if I can end with a \"wild idea\" off \nthe top of my head, maybe even editors should always assume the same \nencoding/decoding, but there be a convenient \"encoding explorer\" as part of the \nplatform that would give an easy way to view files, determine effects of \nchanging encodings (and fonts!), and save new interpretation. Well, just a \nthought. \n\nThanks, hope these ramblings are clear enough so they can be distilled to the \nuse-case or \"encoding principle\" level. (Glad I\u0027m not writing the spec :)\n",
    "See bug 3970 for a request to allow configurable line delimeters.\nIf we allow this, it should be supported at the same granularity as the \nencoding settings (see Kai\u0027s comment).",
    "A new revision of the improved file encoding proposal has been made available\noff the Platform/Core web page:\n\nhttp://dev.eclipse.org/viewcvs/index.cgi/%7Echeckout%7E/platform-core-home/dev.html#plan_current\n\nComments are welcome and should be made on the Platform/Core development list\n(platform-core-dev@eclipse.org) or this PR.\n",
    "While I appreciate the desire to simplify usage, I feel the suggested new plan \n(per-project encodings, with some attempt to discover encodings automatically) \nis a step backwards from the previous one (per-project, folder, and file \nencoding settings).\n\nFirst, there are good reasons to have documents with multiple encodings in a \nsingle project.  For example, suppose I want to ship a product on Windows, Mac, \nand UNIX.  I have README or other files with some requirements for non-ASCII \ncharacters.  For Windows, I might want to use the Windows variant of Latin-1, \nCP 1252.  For Mac, I might want to use Mac Roman.  For UNIX, I might use ISO \nLatin-1.  I might also have Japanese variants in Shift-JIS, Chinese in Big-5, \nand other Asian variants.  These variants are necessary, because a large number \nof sites do not have Unicode system locales - many do not even have Unicode \nlocales installed.\n\nSecond, note that discovering an encoding by reading a file is difficult, and \npotentially expensive (for files not marked with Unicode BOMs).  How much of a \nfile would one read before deciding one knew the encoding?  What if my README \nfile contained 2K or more of 7-bit ASCII text before a section that contained \nsome symbols, or some accented or Asian characters?\n\nThird, the proposal seems to assume that certain file types imply certain \nencodings - notably, that XML should be in UTF-8.  This is not always the \ncase.  There are times when we want to use Latin-1 or even 7-bit ASCII for XML \n(using XML character references for all characters outside those ranges), for \ncompatibility with transport mechanisms and older code that cannot handle UTF-\n8.  As an extreme case, consider sending an XML document through a legacy \nprotocol that is not 8-bit clean.",
    "Sorry for the long \u0027comment\u0027 ... this is the description of at contribution I\u0027d \nlike to propose to see if helpful. It represents (after much refactoring) some \ncode taht we\u0027ve been using to do encoding/decoding on previous products based on \nEclipse 2.x stream. I\u0027ve tried to keep is similar to current spec\u0027s and \nproposals I\u0027ve read, but I suspect much work still needs to be done there, so \nthis is something of a \"stand alone\" version. \n\n\n\n\n\nExtensible Content Sensitive Encoding\nContact: David M. Williams\ndavid_williams@us.ibm.com\n919-254-0362\n\nThe attached contribution (to follow) provides the ability to \"peek\" inside a \nfile to determine its appropriate decoding. This is required for files for which \nthis are common and \"industry standard\" rules ... such as UnicodeStreams, XML, \nJSP, DTDs, HTML and CSS.-- and this \u0027readme\u0027 file is contained in the zip file, \nas the package.html file under the primary project, com.ibm.encoding.resource\n\n\nIt complements some of the work that\u0027s been going on with the Platform and Text \nteams, since I don\u0027t think the \"detector\" part of that spec has been \nimplemented, so I hope this code can save them some effort, as well as provide a \ngood \"test case\" if the ideas really work. This contribution focus entirely on \nthe content sensitive part of the requirements .. there\u0027s other cases and other \nfile types for which the content does not even give a hint as to the encoding. \nThere are \"hooks\" in my code where that work can be tied into so, when \nappropriate, the algorithms can go and look up the settings according to user \nsettings.\n\n\n\n\n\nFor anyone taking a look at this code, here\u0027s an outline of the places to start \n... the primary packages and classes.\n\ncom.ibm.encoding.resource \n\n     CodedReaderCreator -- creates a Reader with correct encoding set to read \ncharacters. \n\n     CodedStreamCreator -- creates a ByteOutputStream, the bytes of which being \ncorrectly encoding for storing. \n\ncom.ibm.encoding.resource.contentspecific \n\ncontains \"detectors\" and infrastructure for XML, JSPs, HTML, CSS, and DTDs. The \ninfrastructure simple means the mechanisms to associate the right encoding rules \n(detectors) with the right content type. This infrastructure makes use of \n\"contentTypeIdentifers\" which I contributed as an append to another bugzilla, \nbut have re-included in this package for convenience. This means it should work \neven for .project files which contains NL characters. NOTE: the base eclipse has \nsaid they might provide one for XML, and I\u0027m sure there might be hesitancy to \ninclude the others, for JSP, HTML, CSS, and DTDs, but I felt obligated to \ninclude them for two reasons: 1) just in case there is a desire (from community \nor others) to include them, and 2) You can\u0027t adequately test the design with \njust one case, so they will be helpful at least for that.\n\nWhile there\u0027s many unit tests which might prove helpful in understanding and \nverifying the code, its really hard to \"see\" and appreciate the results, without \nan editor, or some other to visually see the correct characters are there.\n\n\n\n\n\nHow to use with an editor\n\nAn outline of how to changed basic text editor (via file buffers) to have the \ncontent read and written correctly. Given the following sorts of changes, the \nbasic text editor can open XML, JSP, HTML, CSS, and DTD files no matter what \ntheir \"internal\" encoding is (well, as long as its fairly well formed, and is \nsupported by the VM). [And, yes, you heard right, that\u0027s the same editor, the \neditor shouldn\u0027t have much to do with encoding/decoding, since that\u0027s a \"model \nlevel\" responsibility.]\n\n\nResourceTextFileBuffer\n\ncommitFileBufferContent\n\n\t\t\tCodedStreamCreator codedStreamCreator \u003d new CodedStreamCreator();\n\t\t\tcodedStreamCreator.set(fFile.getName(), fDocument.get());\n\t\t\tByteArrayOutputStream byteStream \u003d codedStreamCreator.\ngetCodedByteArrayOutputStream();\n\t\t\tInputStream stream \u003d new ByteArrayInputStream(byteStream.\ntoByteArray());\n\t\t\t//InputStream stream\u003d new ByteArrayInputStream(fDocument.get().\ngetBytes(encoding));\n\n\ninitializeFileBufferContent\n\n\t\t    CodedReaderCreator codedReaderCreator \u003d new CodedReaderCreator(fFile);\n\t\t    fEncoding \u003d codedReaderCreator.getEncodingMemento().\ngetJavaCharsetName();\n\t\t    //fEncoding\u003d fFile.getPersistentProperty(ENCODING_KEY);\n\n\nThere\u0027s actually simpler ways than the above code indicates, but it would \nrequire more modification of the existing code, so I fit into what\u0027s there as \neasily as possible. (Also, I might note, these simple changes don\u0027t begin to \ncover error conditions, or eliminate incorrect messages. It does not update that \nfile property or file states, or anything like that.\n\nNote, the code as contributed was based on I20040304. I know of some bugs still \nin it, and want to do more cleanup and documentation, but have finally gotten it \nto the point that I think others would study it and give any comments they\u0027d \nfind constructive.\n\nPossible Issues\n\nMaybe its the state of the current code, but it seems lots of objects need to \nknow and have the file encoding set and synchronized, all at just the right \ntimes. This seems very confusing to me. I\u0027ve tried to \"centralize\" all the \nencoding \"intellegence\" and algorithms in just a few classes. AND NOTE: these \ncurrent classes could be \"left seperate\" or encapuslated under IFile/IStorage, \nbut am not sure of the advantage of that, exactly ... maybe I\u0027ve just gotten too \nused to thinking of IFile and IStorage as binary providing objects.\n\nI\u0027ve argued elsewhere, and still believe that the simple \u0027charset\u0027 string is not \nenough to know how to re-write a file. In many well known cases, it depends on \nhow it was read. For example, a file can be UTF-8 with or without the 3 Byte \nBOM. So, I advocate a simple \"encodingMemento\" be available that can provide \ndetailed information about how a stream was decoded, and in a good system, this \ninformation would influence how it was later encoded again. Even if the base \ndidn\u0027t want to support such elaborate stategies, if getCharsetMemento() returned \na mememto with one method, getCharset(), then this would seem to leave the way \nopen for other to implement more elaborate strategies as required by their \nproducts.\n\nContentTypeIdentifier. I haven\u0027t heard any direct feedback, about my original \nproposal, but have seen discussions of even more complicated collections of \narbitrary information about the contents of a file. That might be a nice thing \nto have, and might work, but its hard for me to envision how that works without \na low-level identifiction first. There\u0027s a certain order that, seems to me, to \nbe required. It might be summarized as before you can determine complex type \ninformation, you need to know how to decode the file, and before you can decode \nthe file you need to know what type the file is [and sometimes the file\u0027s \nextension is not enough to determine its type.\n\nMaybe I\u0027m over-reacting, but I fear if there\u0027s lots of different, uncoordinated \ninformation about files that are all need to be saved and modified, all in just \nthe right order, that Eclipse will end up appearing like some \"proprietary\" IDE \n... meaning that things work fine when in Eclipse, but not work fine when \nexported. As a simple example, if a user (or program) sets a documents encoding \nto UTF-16, but due to project settings, or some timing problem it actually gets \nsaved as UTF-8, then that file would be unreadable. I prefer the central object \nthat knows, rather than many objects that all have to stay in synch. So ... just \na \"possible issue\". \n",
    "Created an attachment (id\u003d8384)\nProposed Contribution to handle encoding based on a file\u0027s content\n\nThis zip file contains 7 projects, but only one is the \u0027primary\u0027 one,\ncom.ibm.encoding.resource. I20040304 is the last build I\u0027ve looked at, so hope\nI\u0027m not way behind :) and hope some use can be made of this contribution.\nThanks. ",
    "New file encoding support is now available in the latest builds. \nMoving to Rafael for comment/closure.",
    "I\u0027ve looked at I200404131323. Compile run and debug look fine, but I got errors \non Compare and Search function. I\u0027ve tried following scenario.\n1-create java program on RHEL 3.0WS Japanese locale (workbench default encoding \nis EUC-JP).\n2-create java project on windows2003 (workench default encoding is MS932), and \nchange EUC-JP at the project properties \u003e info page\n3-import the java programs into the project\n4-try run, debug, search, edit and compare from local history\n\nRun and debug look fine. Search with Japanese text pop up an error and .log \nattached below. Compare indicated unmodified Japanese text since Japanese text \nare garbled in original file. it looks like it\u0027s encoded by EUC-JP.\n\n!SESSION 4 20, 2004 19:16:10.500 -----------------------------------------------\njava.fullversion\u003dJ2RE 1.4.2 IBM Windows 32 build cndev-20040322 (JIT enabled: \njitc)\nBootLoader constants: OS\u003dwin32, ARCH\u003dx86, WS\u003dwin32, NL\u003dja_JP\n!ENTRY org.eclipse.core.runtime 4 2 4 20, 2004 19:16:10.516\n!MESSAGE An internal error occurred during: \"Search for References\".\n!STACK 0\njava.lang.NullPointerException\n\tat org.eclipse.search2.internal.ui.SearchView.queryFinished\n(SearchView.java:449)\n\tat org.eclipse.search2.internal.ui.QueryManager.fireFinished\n(QueryManager.java:108)\n\tat org.eclipse.search2.internal.ui.QueryManager.queryFinished\n(QueryManager.java:126)\n\tat org.eclipse.search2.internal.ui.InternalSearchUI.searchJobFinished\n(InternalSearchUI.java:151)\n\tat org.eclipse.search2.internal.ui.InternalSearchUI.access$1\n(InternalSearchUI.java:149)\n\tat \norg.eclipse.search2.internal.ui.InternalSearchUI$InternalSearchJob.run\n(InternalSearchUI.java:133)\n\tat org.eclipse.core.internal.jobs.Worker.run(Worker.java:62)\n\n ",
    "Created an attachment (id\u003d9688)\nsample java program encoded by EUC-JP\n",
    "Created an attachment (id\u003d9689)\nscreen shot of Compare\n",
    "Please file separate bug reports against the search and compare components and list detailled steps for \nhow to reproduce the problem.",
    "ok, I\u0027ve just filed bug 59228 for Search and bug 59232 for Compare problem.",
    "Plan items must target release.",
    "Many of the issues raised in this bug or the platform-core dev list were\naddressed (at least at the Core level). Encoding doc to be updated accordingly\nsoon. Please open separate bug against Platform/Core for any encoding-related\nissues that may arise.\n \nInteresting starting points:\n- IContentTypeManager#getDescriptionFor\n- IEncodedStorage#getCharset\n- IFile#getCharset([boolean])\n- IFile#getContentDescription\n- IContentDescription#BYTE_ORDER_MARK/CHARSET\n\nThanks for the great feedback.",
    "Closing."
  ],
  "commentCreationDates": [
    "2003-05-21T18:31:28+02:00",
    "2003-05-21T18:32:33+02:00",
    "2003-06-06T16:38:07+02:00",
    "2003-06-12T10:49:37+02:00",
    "2003-06-16T17:17:06+02:00",
    "2003-06-17T02:41:45+02:00",
    "2003-06-17T02:46:31+02:00",
    "2003-06-26T03:17:41+02:00",
    "2003-06-27T17:52:16+02:00",
    "2004-02-23T18:54:42+01:00",
    "2004-02-23T19:54:24+01:00",
    "2004-03-08T06:52:05+01:00",
    "2004-03-08T07:00:30+01:00",
    "2004-04-15T15:33:17+02:00",
    "2004-04-20T12:47:44+02:00",
    "2004-04-20T12:48:43+02:00",
    "2004-04-20T12:54:48+02:00",
    "2004-04-20T12:56:46+02:00",
    "2004-04-20T14:09:46+02:00",
    "2004-04-23T17:57:10+02:00",
    "2004-04-30T22:43:23+02:00",
    "2004-05-07T22:57:42+02:00"
  ],
  "traces": [
    {
      "exceptionType": "java.lang.NullPointerException",
      "elements": [
        {
          "method": "org.eclipse.search2.internal.ui.SearchView.queryFinished",
          "source": "SearchView.java:449"
        },
        {
          "method": "org.eclipse.search2.internal.ui.QueryManager.fireFinished",
          "source": "QueryManager.java:108"
        },
        {
          "method": "org.eclipse.search2.internal.ui.QueryManager.queryFinished",
          "source": "QueryManager.java:126"
        },
        {
          "method": "org.eclipse.search2.internal.ui.InternalSearchUI.searchJobFinished",
          "source": "InternalSearchUI.java:151"
        },
        {
          "method": "org.eclipse.search2.internal.ui.InternalSearchUI.access$1",
          "source": "InternalSearchUI.java:149"
        },
        {
          "method": "org.eclipse.search2.internal.ui.InternalSearchUI$InternalSearchJob.run",
          "source": "InternalSearchUI.java:133"
        },
        {
          "method": "org.eclipse.core.internal.jobs.Worker.run",
          "source": "Worker.java:62"
        }
      ],
      "number": 0,
      "commentIndex": 14,
      "bugId": "37933",
      "date": "2004-04-20T12:47:44+02:00",
      "product": "Platform",
      "component": "Resources",
      "severity": "enhancement"
    }
  ],
  "groupId": "37933",
  "bugId": "37933",
  "date": "2003-05-21T18:31:28+02:00",
  "product": "Platform",
  "component": "Resources",
  "severity": "enhancement"
}