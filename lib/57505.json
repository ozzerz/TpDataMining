{
  "comments": [
    "It is still possible to schedule a WorkbenchJob after the Workbench has shut \ndown if it shuts down after shouldSchedule() is called. We should implement a \nmethod scheduleIfPossible which only schedules when the Workbench is running.\n\nWe need to check all implementors of Workbench job for this. Unfortunately \nschedule(delay) is final so we can\u0027t test there.",
    "I am seeing the following call stack at the end of a test I am trying to run:\njava.lang.IllegalStateException: Job manager has been shut down.\n\tat org.eclipse.core.internal.jobs.JobManager.schedule\n(JobManager.java:639)\n\tat org.eclipse.core.internal.jobs.InternalJob.schedule\n(InternalJob.java:268)\n\tat org.eclipse.core.runtime.jobs.Job.schedule(Job.java:363)\n\tat org.eclipse.ui.views.markers.internal.TableContentProvider.doUpdate\n(TableContentProvider.java:396)\n\tat org.eclipse.ui.views.markers.internal.TableContentProvider.access$4\n(TableContentProvider.java:344)\n\tat org.eclipse.ui.views.markers.internal.TableContentProvider$3.run\n(TableContentProvider.java:182)\n\tat org.eclipse.ui.views.markers.internal.RestartableJob$2.run\n(RestartableJob.java:86)\n\tat org.eclipse.core.internal.jobs.Worker.run(Worker.java:62)\n\n\nThe test is not yet in HEAD.  Will annotate as more information is available.\n",
    "Talking to John the issue here is that we are still running things after \nshutdown of the UI.\n\nRather than add a method to WorkbenchJob to check for this views that have \nthis problem should not even attempt updates if they are disposed.\n\nIf they are not disposed there is a bigger issue than a check here. ",
    "*** Bug 57533 has been marked as a duplicate of this bug. ***",
    "*** Bug 57719 has been marked as a duplicate of this bug. ***",
    "Closing this bug as we should be looking at specific problems rather than a \ngeneral solution.\n\nDeb please advise if the test is still an issue an if so please log a PR \nagainst the MarkersView.",
    "This bug is still valid. It is very hard for views to prevent things from being\nscheduled after the job manager is shut down. I\u0027ll explain:\n\nPROBLEM 1\n\nThe current API requires breaking a job into smaller jobs for the purposes of\nlocking and unlocking shared resources. These chains of jobs are really a single\njob, and terminating the job manager before the chain of jobs has completed has\nthe same effect as terminating the thread: resources will be leaked since the\njob cannot clean up after itself.\n\nFor a concrete example, consider this:\n\nUIJob1 {\n  allocate SWT resource X\n}\nWorkbenchJob2 {\n  create temporary file Y\n}\nBackgroundJob3 {\n  do some long-running background operation that can be safely cancelled\n}\nWorkbenchJob4 {\n  delete Y\n}\nUIJob5 {\n  release resource X\n}\n\nThese jobs are chained together using a sequence of JobChangeListeners. Although\nthe JobManager will allow any one of them to run to completion, the real\nrequirement is for the entire chain to run to completion once started.\n\nThis could be solved in several ways:\nA) Offer a composite job class (bug 53652) -- and allow the chain to run to\ncompletion even if some of the sub-jobs are scheduled after the job manager is\nshut down.\n\nB) Offer a way to schedule a job on some lock, and release that lock before the\njob terminates -- and provide API to wait on a lock (or the UI thread), but to\nhide the waiting job from the progress view, as though it had been scheduled on\nthe lock but had not started running yet. This would allow the above algorithm\nto be coded using a single Job.\n\nC) Offer a semaphore to prevent the Job manager from shutting down. When the Job\nmanager shuts down, it will cancel all active jobs but will continue to allow\nnew jobs to be scheduled until the semaphore is released. In this way, a chain\nof Jobs could grab the semaphore when they start and release it when they finish.\n\nAll of the IllegalStateExceptions I\u0027ve seen are being caused by exactly this\nsort of job. However, this is not critical problem right now since most of these\n\"cleanup\" jobs are updating reference counts or protecting against memory leaks,\nwhich make less of a difference at the end of the application.\n\n\nPROBLEM 2.\n\nIn order for a view to protect against starting a job after the job manager\nterminates, it would probably contain code that looks like this:\n\n1. if (PlatformUI.isWorkbenchRunning()) {\n2.   myJob.schedule();\n3. }\n\nHowever, this does not protect against the race condition of the Job manager\nterminating between lines 1 and 2. (In which case an IllegalStateException will\nstill be thrown) What is really needed is something like this:\n\n1. JobManager.preventTermination();\n2. try {\n3.   if (PlatformUI.isWorkbenchRunning()) {\n4.     myJob.schedule();\n5.   }\n6. } finally {\n7.   JobManager.allowTermination()\n8. }\n\nThis could also be solved (more elegantly, IMHO) using the \"scheduleIfPossible\"\nAPI suggested in the original PR.\n\n",
    "When the job manager is shut down so is the workbench - that is the point of \nsaying won\u0027t fix - the fact that it got as far as this is the real problem. \nYour view needs to make sure that when its dispose methods are called that all \njobs it is using are cancelled as well. You won\u0027t be able to do anything \nwithout a workbench.",
    "Yes.. you need to cancel the job, but you can\u0027t just terminate it in the middle\nof running. The job needs to check the cancelled flag regularly, release any\nresources it is holding onto, and terminate cleanly.\n\nThe exceptions we are getting are a result of jobs that are trying to terminate\nthemselves but cannot spawn the sub-jobs they need to terminate cleanly.",
    "At the point where the exception occurs, the workbench and the core runtime have\nalready been shut down.  It is too late at this point to do anything\ninteresting. If the job (or chain of jobs) has essential work to do, then the\nworkbench shutdown needs to be delayed until they complete (for example by\ncanceling and then waiting until the last essential job completes from the\nview\u0027s dispose method). If the jobs are not doing essential work (such as\nupdating a view), then they don\u0027t need to be scheduled in this situation. You\ncould use Platform.isRunning() or Workbench.isRunning() to identify this case,\nor just catch the runtime exception that is returned from schedule() (which\nwould be safer, since the platform could shutdown after isRunning() returns and\nbefore schedule() occurs).",
    "Good suggestions, John. Waiting for the job to terminate in the dispose method\nwill solve the termination issues, and catching the exception will avoid the\nconcurrency issues for non-essential cleanup.\n\nIt would be useful if there was a generic solution for this so that each view\nauthor doesn\u0027t need to figure it out for themself. I would suggest the following\nchanges:\n\n1. Rather than making each view responsible for cancelling its jobs and waiting\nfor them to terminate, make this the default behavior for any job scheduled\nusing IWorkbenchSiteProgressService.\n\n2. Provide some sort of shorthand to declare a \"non-essential\" job. I believe\nthis was the original intention of WorkbenchJob. For such jobs, the user should\nnot need to wrap every call with a check to see if the workbench exists --\nattempting to schedule or join it will simply result in a NOP if it can\u0027t start.\n\nReducing severity to \"enhancement\" since John\u0027s suggestions provide a reasonable\nworkaround.\n",
    "Not for 3.0 as the API freeze is this week.",
    "John has changed schedule() to be non-final. We should investigate what we can \ndo now.",
    "I\u0027ve filed bug 58748 for comment 10",
    "Job.schedule() is final in HEAD so there is nothing more we can do here."
  ],
  "commentCreationDates": [
    "2004-04-05T22:19:43+02:00",
    "2004-04-06T15:06:03+02:00",
    "2004-04-06T15:19:06+02:00",
    "2004-04-06T16:45:53+02:00",
    "2004-04-07T16:07:26+02:00",
    "2004-04-12T16:09:53+02:00",
    "2004-04-13T03:47:15+02:00",
    "2004-04-13T14:07:14+02:00",
    "2004-04-13T19:53:37+02:00",
    "2004-04-13T20:03:37+02:00",
    "2004-04-13T20:49:06+02:00",
    "2004-04-14T13:53:31+02:00",
    "2004-04-14T22:07:26+02:00",
    "2004-04-15T23:13:16+02:00",
    "2004-04-27T15:42:43+02:00"
  ],
  "traces": [
    {
      "exceptionType": "java.lang.IllegalStateException",
      "message": "Job manager has been shut down.",
      "elements": [
        {
          "method": "org.eclipse.core.internal.jobs.JobManager.schedule",
          "source": "JobManager.java:639"
        },
        {
          "method": "org.eclipse.core.internal.jobs.InternalJob.schedule",
          "source": "InternalJob.java:268"
        },
        {
          "method": "org.eclipse.core.runtime.jobs.Job.schedule",
          "source": "Job.java:363"
        },
        {
          "method": "org.eclipse.ui.views.markers.internal.TableContentProvider.doUpdate",
          "source": "TableContentProvider.java:396"
        },
        {
          "method": "org.eclipse.ui.views.markers.internal.TableContentProvider.access$4",
          "source": "TableContentProvider.java:344"
        },
        {
          "method": "org.eclipse.ui.views.markers.internal.TableContentProvider$3.run",
          "source": "TableContentProvider.java:182"
        },
        {
          "method": "org.eclipse.ui.views.markers.internal.RestartableJob$2.run",
          "source": "RestartableJob.java:86"
        },
        {
          "method": "org.eclipse.core.internal.jobs.Worker.run",
          "source": "Worker.java:62"
        }
      ],
      "number": 0,
      "commentIndex": 1,
      "bugId": "57505",
      "date": "2004-04-06T15:06:03+02:00",
      "product": "Platform",
      "component": "UI",
      "severity": "enhancement"
    }
  ],
  "groupId": "57505",
  "bugId": "57505",
  "date": "2004-04-05T22:19:43+02:00",
  "product": "Platform",
  "component": "UI",
  "severity": "enhancement"
}